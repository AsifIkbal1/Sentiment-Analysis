{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# English ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nfrom transformers import TFAutoModelForSequenceClassification\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom scipy.special import softmax\nimport csv\nimport urllib.request\n\n# Preprocess text (username and link placeholders)\ndef preprocess(text):\n    new_text = []\n \n \n    for t in text.split(\" \"):\n        t = '@user' if t.startswith('@') and len(t) > 1 else t\n        t = 'http' if t.startswith('http') else t\n        new_text.append(t)\n    return \" \".join(new_text)\n\n\ntask='sentiment'\nMODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\n# download label mapping\nlabels=[]\nmapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\nwith urllib.request.urlopen(mapping_link) as f:\n    html = f.read().decode('utf-8').split(\"\\n\")\n    csvreader = csv.reader(html, delimiter='\\t')\nlabels = [row[1] for row in csvreader if len(row) > 1]\n\n# PT\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL)\nmodel.save_pretrained(MODEL)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:53:34.660198Z","iopub.execute_input":"2024-02-04T22:53:34.660533Z","iopub.status.idle":"2024-02-04T22:53:50.124229Z","shell.execute_reply.started":"2024-02-04T22:53:34.660505Z","shell.execute_reply":"2024-02-04T22:53:50.122821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_analysis(text):\n    text = text\n    text = preprocess(text)\n    encoded_input = tokenizer(text, return_tensors='pt')\n    output = model(**encoded_input)\n    scores = output[0][0].detach().numpy()\n    scores = softmax(scores)\n\n    ranking = np.argsort(scores)\n    ranking = ranking[::-1]\n    for i in range(scores.shape[0]):\n        l = labels[ranking[i]]\n        s = scores[ranking[i]]\n        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:53:50.125976Z","iopub.execute_input":"2024-02-04T22:53:50.12647Z","iopub.status.idle":"2024-02-04T22:53:50.135282Z","shell.execute_reply.started":"2024-02-04T22:53:50.12644Z","shell.execute_reply":"2024-02-04T22:53:50.133572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentiment_analysis(\"I need to buy groceries after work.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:53:50.136584Z","iopub.execute_input":"2024-02-04T22:53:50.13696Z","iopub.status.idle":"2024-02-04T22:53:50.35942Z","shell.execute_reply.started":"2024-02-04T22:53:50.136913Z","shell.execute_reply":"2024-02-04T22:53:50.358177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Arabic ","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nmodel = pipeline('text-classification', model='CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:53:50.3619Z","iopub.execute_input":"2024-02-04T22:53:50.362226Z","iopub.status.idle":"2024-02-04T22:54:14.295517Z","shell.execute_reply.started":"2024-02-04T22:53:50.362202Z","shell.execute_reply":"2024-02-04T22:54:14.293977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def arabic_sentiment_analysis(text):\n    result = model(text)  \n    for entry in result:\n        label = entry['label']\n        score = entry['score']\n        print(f'Text: \"{text}\"')\n        print(f'Sentiment Label: {label}')\n        print(f'Sentiment Score: {score}')\n        print('---')","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:54:14.296912Z","iopub.execute_input":"2024-02-04T22:54:14.29748Z","iopub.status.idle":"2024-02-04T22:54:14.302908Z","shell.execute_reply.started":"2024-02-04T22:54:14.297451Z","shell.execute_reply":"2024-02-04T22:54:14.30192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arabic_sentiment_analysis(\"انا حزين\")","metadata":{"execution":{"iopub.status.busy":"2024-02-04T22:54:14.304359Z","iopub.execute_input":"2024-02-04T22:54:14.304929Z","iopub.status.idle":"2024-02-04T22:54:14.384509Z","shell.execute_reply.started":"2024-02-04T22:54:14.304902Z","shell.execute_reply":"2024-02-04T22:54:14.383188Z"},"trusted":true},"execution_count":null,"outputs":[]}]}